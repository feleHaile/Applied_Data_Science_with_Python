{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "_You are currently looking at **version 1.0** of this notebook. To download notebooks and datafiles, as well as get help on Jupyter notebooks in the Coursera platform, visit the [Jupyter Notebook FAQ](https://www.coursera.org/learn/python-text-mining/resources/d9pwm) course resource._\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2 - Introduction to NLTK\n",
    "\n",
    "In part 1 of this assignment you will use nltk to explore the Herman Melville novel Moby Dick. Then in part 2 you will create a spelling recommender function that uses nltk to find words similar to the misspelling. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - Analyzing Moby Dick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# If you would like to work with the raw text you can use 'moby_raw'\n",
    "with open('moby.txt', 'r') as f:\n",
    "    moby_raw = f.read()\n",
    "    \n",
    "# If you would like to work with the novel in nltk.Text format you can use 'text1'\n",
    "moby_tokens = nltk.word_tokenize(moby_raw)\n",
    "text1 = nltk.Text(moby_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1\n",
    "\n",
    "How many tokens (words and punctuation symbols) are in text1?\n",
    "\n",
    "*This function should return an integer.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "254989"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def example_one():\n",
    "    \n",
    "    return len(nltk.word_tokenize(moby_raw)) # or alternatively len(text1)\n",
    "\n",
    "example_one()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2\n",
    "\n",
    "How many unique tokens (unique words and punctuation) does text1 have?\n",
    "\n",
    "*This function should return an integer.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20755"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def example_two():\n",
    "    \n",
    "    return len(set(nltk.word_tokenize(moby_raw))) # or alternatively len(set(text1))\n",
    "\n",
    "example_two()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3\n",
    "\n",
    "After lemmatizing the verbs, how many unique tokens does text1 have?\n",
    "\n",
    "*This function should return an integer.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16900"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def example_three():\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized = [lemmatizer.lemmatize(w,'v') for w in text1]\n",
    "\n",
    "    return len(set(lemmatized))\n",
    "\n",
    "example_three()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "\n",
    "What is the lexical diversity of the given text input? (i.e. ratio of unique tokens to the total number of tokens)\n",
    "\n",
    "*This function should return a float.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08139566804842562"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def answer_one():\n",
    "    \n",
    "    \n",
    "    return len(set(nltk.word_tokenize(moby_raw)))/len(nltk.word_tokenize(moby_raw))\n",
    "\n",
    "answer_one()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "\n",
    "What percentage of tokens is 'whale'or 'Whale'?\n",
    "\n",
    "*This function should return a float.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.004125668166077752"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def answer_two():\n",
    "    tokens = nltk.word_tokenize(moby_raw)\n",
    "    \n",
    "    return (tokens.count('Whale') + tokens.count('whale'))/len(tokens)\n",
    "\n",
    "answer_two()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "\n",
    "What are the 20 most frequently occurring (unique) tokens in the text? What is their frequency?\n",
    "\n",
    "*This function should return a list of 20 tuples where each tuple is of the form `(token, frequency)`. The list should be sorted in descending order of frequency.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(',', 0.07531305271992125),\n",
       " ('the', 0.053786633933228495),\n",
       " ('.', 0.028660059845718833),\n",
       " ('of', 0.025542278294357797),\n",
       " ('and', 0.023569644180729366),\n",
       " ('a', 0.017824298303064055),\n",
       " ('to', 0.01770664616905043),\n",
       " (';', 0.016365411841295113),\n",
       " ('in', 0.015326151324174768),\n",
       " ('that', 0.01167893516975242),\n",
       " ('his', 0.009643553251316724),\n",
       " ('it', 0.008612136209797285),\n",
       " ('I', 0.008223884167552325),\n",
       " ('!', 0.006929710693402461),\n",
       " ('is', 0.0067532324923820245),\n",
       " ('--', 0.0067179368521779375),\n",
       " ('with', 0.006506163010953414),\n",
       " ('he', 0.006502241273152959),\n",
       " ('was', 0.006427728254944331),\n",
       " ('as', 0.006353215236735702)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def answer_three():\n",
    "    tokens = nltk.word_tokenize(moby_raw)\n",
    "    ntokens = len(tokens)\n",
    "    uni_tokens = pd.Series(list(map(lambda x:tokens.count(x)/ntokens, list(set(tokens)))), \n",
    "                           index = list(set(tokens)))\n",
    "    uni_tokens = uni_tokens.sort_values(ascending = False).head(20)\n",
    "    \n",
    "    \n",
    "    return list(zip(uni_tokens.index, uni_tokens.iloc[:]))\n",
    "\n",
    "answer_three()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "\n",
    "What tokens have a length of greater than 5 and frequency of more than 150?\n",
    "\n",
    "*This function should return a sorted list of the tokens that match the above constraints. To sort your list, use `sorted()`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'[False False True ... False False False] not found in axis'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-b5281b4b78e1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0muni_tokens\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0manswer_four\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-28-b5281b4b78e1>\u001b[0m in \u001b[0;36manswer_four\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m                            index = list(set(tokens)))\n\u001b[0;32m      5\u001b[0m     \u001b[0mdroprows\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mFalse\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m5\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mTrue\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0muni_tokens\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0muni_tokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0muni_tokens\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdroprows\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0muni_tokens\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\xinda du\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   3414\u001b[0m         return super(Series, self).drop(labels=labels, axis=axis, index=index,\n\u001b[0;32m   3415\u001b[0m                                         \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3416\u001b[1;33m                                         inplace=inplace, errors=errors)\n\u001b[0m\u001b[0;32m   3417\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3418\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mSubstitution\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0m_shared_doc_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\xinda du\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   3109\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32min\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3110\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3111\u001b[1;33m                 \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_drop_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3112\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3113\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\xinda du\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_drop_axis\u001b[1;34m(self, labels, axis, level, errors)\u001b[0m\n\u001b[0;32m   3141\u001b[0m                 \u001b[0mnew_axis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3142\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3143\u001b[1;33m                 \u001b[0mnew_axis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3144\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0maxis_name\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnew_axis\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3145\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\xinda du\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, errors)\u001b[0m\n\u001b[0;32m   4402\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'ignore'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4403\u001b[0m                 raise KeyError(\n\u001b[1;32m-> 4404\u001b[1;33m                     '{} not found in axis'.format(labels[mask]))\n\u001b[0m\u001b[0;32m   4405\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4406\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: '[False False True ... False False False] not found in axis'"
     ]
    }
   ],
   "source": [
    "def answer_four():\n",
    "    tokens = nltk.word_tokenize(moby_raw)\n",
    "    uni_tokens = pd.Series(list(map(lambda x:tokens.count(x), list(set(tokens)))), \n",
    "                           index = list(set(tokens)))\n",
    "    droprows = [False if len(x) > 5 else True for x in uni_tokens.index]\n",
    "    uni_tokens = uni_tokens.drop(droprows, axis = 0)\n",
    "    \n",
    "    return uni_tokens\n",
    "\n",
    "answer_four()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "\n",
    "Find the longest word in text1 and that word's length.\n",
    "\n",
    "*This function should return a tuple `(longest_word, length)`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"twelve-o'clock-at-night\", 23)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def answer_five():\n",
    "    token_lens = [len(x) for x in text1]\n",
    "    \n",
    "    return (text1[token_lens.index(max(token_lens))], max(token_lens))\n",
    "\n",
    "answer_five()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6\n",
    "\n",
    "What unique words have a frequency of more than 2000? What is their frequency?\n",
    "\n",
    "\"Hint:  you may want to use `isalpha()` to check if the token is a word and not punctuation.\"\n",
    "\n",
    "*This function should return a list of tuples of the form `(frequency, word)` sorted in descending order of frequency.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cleanliest',\n",
       " 'not',\n",
       " 'ANNALS',\n",
       " 'VARIOUS',\n",
       " 'junks',\n",
       " 'pedestals',\n",
       " 'squares',\n",
       " 'Steel',\n",
       " 'stumped',\n",
       " 'Men',\n",
       " 'serfs',\n",
       " 'appeals',\n",
       " 'woven',\n",
       " 'SOMEWHERE',\n",
       " 'panel',\n",
       " 'bodices',\n",
       " 'inventors',\n",
       " 'propulsion',\n",
       " 'countersunk',\n",
       " 'passed',\n",
       " 'hated',\n",
       " 'widening',\n",
       " 'visit',\n",
       " 'cutlets',\n",
       " 'gazettes',\n",
       " 'Art',\n",
       " 'helping',\n",
       " 'professions',\n",
       " 'tastes',\n",
       " 'dabs',\n",
       " 'muffins',\n",
       " 'bet',\n",
       " 'swords',\n",
       " 'Mountains',\n",
       " 'gifted',\n",
       " 'tasks',\n",
       " 'disgust',\n",
       " 'came',\n",
       " 'bay',\n",
       " 'snare',\n",
       " 'atrocious',\n",
       " 'Lent',\n",
       " 'disconnected',\n",
       " 'beckoning',\n",
       " 'respectable',\n",
       " 'cuffs',\n",
       " 'vows',\n",
       " 'separated',\n",
       " 'stick',\n",
       " 'sooth',\n",
       " 'seating',\n",
       " 'cottage',\n",
       " 'WAL',\n",
       " 'famed',\n",
       " 'platformed',\n",
       " 'gaseous',\n",
       " 'treadle',\n",
       " 'hinted',\n",
       " 'pray',\n",
       " 'semiweekly',\n",
       " 'bread',\n",
       " 'cheaply',\n",
       " 'whaleship',\n",
       " 'flyin',\n",
       " 'sartainty',\n",
       " 'tool',\n",
       " 'heinousness',\n",
       " 'omnipresence',\n",
       " 'Shooting',\n",
       " 'engineering',\n",
       " 'breasts',\n",
       " 'northern',\n",
       " 'fleeting',\n",
       " 'sheet',\n",
       " 'spasmodic',\n",
       " 'ticks',\n",
       " 'solicitously',\n",
       " 'swept',\n",
       " 'drenched',\n",
       " 'untraceable',\n",
       " 'skirting',\n",
       " 'blindest',\n",
       " 'shelves',\n",
       " 'Texas',\n",
       " 'plain',\n",
       " 'kindling',\n",
       " 'consecrated',\n",
       " 'King',\n",
       " 'unintelligence',\n",
       " 'Marmora',\n",
       " 'rise',\n",
       " 'meanings',\n",
       " 'awake',\n",
       " 'manliness',\n",
       " 'Do',\n",
       " 'thoughtfulness',\n",
       " 'qualified',\n",
       " 'bar',\n",
       " 'panniers',\n",
       " 'marred',\n",
       " 'Aboard',\n",
       " 'beautiful',\n",
       " 'SHOALS',\n",
       " 'begets',\n",
       " 'wheelbarrow',\n",
       " 'dissociated',\n",
       " 'gentlemanly',\n",
       " 'book',\n",
       " 'Deliberately',\n",
       " 'Cluny',\n",
       " 'visual',\n",
       " 'lacings',\n",
       " 'cant',\n",
       " 'paper',\n",
       " 'Cyclades',\n",
       " 'retribution',\n",
       " 'stripping',\n",
       " 'coronation',\n",
       " 'delineations',\n",
       " 'brutal',\n",
       " 'parading',\n",
       " 'plumes',\n",
       " 'FOLLOWING',\n",
       " 'broom',\n",
       " 'contemptible',\n",
       " 'stubbornness',\n",
       " 'filliping',\n",
       " 'boiler',\n",
       " 'immortal',\n",
       " 'disheartening',\n",
       " 'creditor',\n",
       " 'spurred',\n",
       " 'structures',\n",
       " 'bedside',\n",
       " 'boatswain',\n",
       " 'infuriated',\n",
       " 'meeting',\n",
       " 'prescribed',\n",
       " 'albatross',\n",
       " 'foot',\n",
       " 'saddle',\n",
       " 'scorn',\n",
       " 'residue',\n",
       " 'nurseries',\n",
       " 'refugees',\n",
       " 'entablatures',\n",
       " 'Miracle',\n",
       " 'prematurely',\n",
       " 'exist',\n",
       " 'X',\n",
       " 'purr',\n",
       " 'Sultan',\n",
       " 'undeniable',\n",
       " 'quaff',\n",
       " 'ninety',\n",
       " 'Alas',\n",
       " 'emoluments',\n",
       " 'hereditarily',\n",
       " 'colder',\n",
       " 'taffrail',\n",
       " 'consort',\n",
       " 'shattered',\n",
       " 'Beach',\n",
       " 'transfixed',\n",
       " 'rayther',\n",
       " 'Inert',\n",
       " 'unsignifying',\n",
       " 'Farewell',\n",
       " 'convolutions',\n",
       " 'Doubloon',\n",
       " 'Arion',\n",
       " 'disappears',\n",
       " 'fatter',\n",
       " 'undeterred',\n",
       " 'profundities',\n",
       " 'Shadrach',\n",
       " 'arrows',\n",
       " 'liars',\n",
       " 'medicine',\n",
       " 'deformities',\n",
       " 'fascinated',\n",
       " 'barbarous',\n",
       " 'faster',\n",
       " 'comely',\n",
       " 'Hogarth',\n",
       " 'turbid',\n",
       " 'spermacetti',\n",
       " 'reposes',\n",
       " 'freckles',\n",
       " 'peeped',\n",
       " 'case',\n",
       " 'dale',\n",
       " 'furlongs',\n",
       " 'Receiving',\n",
       " 'banterings',\n",
       " 'periodically',\n",
       " 'cart',\n",
       " 'whitewashed',\n",
       " 'darkened',\n",
       " 'economically',\n",
       " 'sportively',\n",
       " 'cylinders',\n",
       " 'engrossed',\n",
       " 'sartin',\n",
       " 'sheets',\n",
       " 'service',\n",
       " 'jungle',\n",
       " 'enthrone',\n",
       " 'bat',\n",
       " 'Freeze',\n",
       " 'calculate',\n",
       " 'honed',\n",
       " 'Doctor',\n",
       " 'admonitory',\n",
       " 'experienced',\n",
       " 'polish',\n",
       " 'search',\n",
       " 'Weep',\n",
       " 'daily',\n",
       " 'possessing',\n",
       " 'braves',\n",
       " 'wool',\n",
       " 'galliot',\n",
       " 'Meshach',\n",
       " 'cows',\n",
       " 'Perth',\n",
       " 'freedom',\n",
       " 'patronising',\n",
       " 'subterraneousness',\n",
       " 'convict',\n",
       " 'nestle',\n",
       " 'Help',\n",
       " 'Already',\n",
       " 'estimate',\n",
       " 'sinners',\n",
       " 'gamesome',\n",
       " 'lbs',\n",
       " 'tonic',\n",
       " 'BELOW',\n",
       " 'beneath',\n",
       " 'metal',\n",
       " 'ulceration',\n",
       " 'Doubts',\n",
       " 'pensive',\n",
       " 'cripple',\n",
       " 'officio',\n",
       " 'Manx',\n",
       " 'Jove',\n",
       " 'imported',\n",
       " 'PEQUOD',\n",
       " 'breezes',\n",
       " 'aggregated',\n",
       " 'situation',\n",
       " 'Morgana',\n",
       " 'rigadig',\n",
       " 'Bashee',\n",
       " 'cells',\n",
       " 'success',\n",
       " 'immaculate',\n",
       " 'morbid',\n",
       " 'sanity',\n",
       " 'eruption',\n",
       " 'inwards',\n",
       " 'steeple',\n",
       " 'multiplied',\n",
       " 'turmoil',\n",
       " 'griffins',\n",
       " 'TWICE',\n",
       " 'sparkling',\n",
       " 'steeds',\n",
       " 'amputations',\n",
       " 'expression',\n",
       " 'billeted',\n",
       " 'loadstone',\n",
       " 'sustains',\n",
       " 'feet',\n",
       " 'maintaining',\n",
       " 'tremor',\n",
       " 'imperilled',\n",
       " 'frighten',\n",
       " 'dropping',\n",
       " 'warp',\n",
       " 'witnessing',\n",
       " 'committing',\n",
       " 'CHINA',\n",
       " 'covers',\n",
       " 'Baliene',\n",
       " 'straightened',\n",
       " 'sorrow',\n",
       " 'dollar',\n",
       " 'twinkling',\n",
       " 'interests',\n",
       " 'frightful',\n",
       " 'Ceylon',\n",
       " 'transit',\n",
       " 'pottery',\n",
       " 'arbitrary',\n",
       " 'Yojo',\n",
       " 'wailing',\n",
       " 'there',\n",
       " 'humorously',\n",
       " 'talons',\n",
       " 'contemplations',\n",
       " 'assurance',\n",
       " 'Australia',\n",
       " 'nightmare',\n",
       " 'skill',\n",
       " 'ISHMAEL',\n",
       " 'apt',\n",
       " 'key',\n",
       " 'likenesses',\n",
       " 'communicating',\n",
       " 'Dorchester',\n",
       " 'orator',\n",
       " 'COWLEY',\n",
       " 'sounded',\n",
       " 'Speak',\n",
       " 'ties',\n",
       " 'bigamist',\n",
       " 'storms',\n",
       " 'SNEEZING',\n",
       " 'Quitting',\n",
       " 'stumbled',\n",
       " 'tastin',\n",
       " 'circumambient',\n",
       " 'rods',\n",
       " 'Remark',\n",
       " 'obtained',\n",
       " 'spluttering',\n",
       " 'Does',\n",
       " 'APPROACHING',\n",
       " 'abstained',\n",
       " 'penalties',\n",
       " 'hide',\n",
       " 'dubious',\n",
       " 'behalf',\n",
       " 'Jury',\n",
       " 'loan',\n",
       " 'campaign',\n",
       " 'orgies',\n",
       " 'overstrained',\n",
       " 'Slope',\n",
       " 'boatmen',\n",
       " 'almost',\n",
       " 'Reference',\n",
       " 'unfulfilments',\n",
       " 'mastheads',\n",
       " 'do',\n",
       " 'waifing',\n",
       " 'lovings',\n",
       " 'vowed',\n",
       " 'argument',\n",
       " 'casualty',\n",
       " 'centres',\n",
       " 'systemized',\n",
       " 'soused',\n",
       " 'district',\n",
       " 'creature',\n",
       " 'heave',\n",
       " 'born',\n",
       " 'hammers',\n",
       " 'sanctorum',\n",
       " 'fastidious',\n",
       " 'accommodated',\n",
       " 'malignantly',\n",
       " 'puff',\n",
       " 'HIS',\n",
       " 'outweigh',\n",
       " 'transcends',\n",
       " 'tropic',\n",
       " 'javelin',\n",
       " 'clap',\n",
       " 'globular',\n",
       " 'anticipated',\n",
       " 'Beware',\n",
       " 'contrast',\n",
       " 'cannibal',\n",
       " 'Helena',\n",
       " 'BREAKWATER',\n",
       " 'panellings',\n",
       " 'marshy',\n",
       " 'superseded',\n",
       " 'Satanic',\n",
       " 'mending',\n",
       " 'marlingspike',\n",
       " 'toder',\n",
       " 'Arrayed',\n",
       " 'pest',\n",
       " 'spades',\n",
       " 'chairs',\n",
       " 'Sam',\n",
       " 'comprehend',\n",
       " 'seethings',\n",
       " 'narrating',\n",
       " 'burst',\n",
       " 'torments',\n",
       " 'considerably',\n",
       " 'denoted',\n",
       " 'Sha',\n",
       " 'Patience',\n",
       " 'Below',\n",
       " 'dribbles',\n",
       " 'knitting',\n",
       " 'Being',\n",
       " 'beer',\n",
       " 'grasp',\n",
       " 'unreasonable',\n",
       " 'Isle',\n",
       " 'covering',\n",
       " 'ROCKS',\n",
       " 'rebellion',\n",
       " 'gouty',\n",
       " 'prefer',\n",
       " 'argosy',\n",
       " 'Oahu',\n",
       " 'conflict',\n",
       " 'controls',\n",
       " 'fight',\n",
       " 'interesting',\n",
       " 'imbedded',\n",
       " 'stairs',\n",
       " 'future',\n",
       " 'swallows',\n",
       " 'marines',\n",
       " 'sinks',\n",
       " 'pack',\n",
       " 'samphire',\n",
       " 'crop',\n",
       " 'solid',\n",
       " 'quadrant',\n",
       " 'affirms',\n",
       " 'barreller',\n",
       " 'imperceptibly',\n",
       " 'eccentric',\n",
       " 'stilly',\n",
       " 'weary',\n",
       " 'objectionable',\n",
       " 'needing',\n",
       " 'ulcerous',\n",
       " 'HATCHWAY',\n",
       " 'dodge',\n",
       " 'cloven',\n",
       " 'tow',\n",
       " 'dramatically',\n",
       " 'wantest',\n",
       " 'Alarmed',\n",
       " 'zodiac',\n",
       " 'chose',\n",
       " 'enter',\n",
       " 'rounding',\n",
       " 'Pythagorean',\n",
       " 'lone',\n",
       " 'clotting',\n",
       " 'child',\n",
       " 'ditchers',\n",
       " 'raise',\n",
       " 'scenes',\n",
       " 'pester',\n",
       " 'Tying',\n",
       " 'arching',\n",
       " 'per',\n",
       " 'sulks',\n",
       " 'untrackably',\n",
       " 'cupidity',\n",
       " 'Hecla',\n",
       " 'stateliest',\n",
       " 'unstarched',\n",
       " 'Nay',\n",
       " 'Dart',\n",
       " 'Wretched',\n",
       " 'susceptible',\n",
       " 'patent',\n",
       " 'fry',\n",
       " 'dared',\n",
       " 'moon',\n",
       " 'harpooneer',\n",
       " 'curiously',\n",
       " 'Meantime',\n",
       " 'boughs',\n",
       " 'overlooking',\n",
       " 'Syrian',\n",
       " 'wee',\n",
       " 'intention',\n",
       " 'palavering',\n",
       " 'pointless',\n",
       " 'traveller',\n",
       " 'staggeringly',\n",
       " 'feed',\n",
       " 'deformed',\n",
       " 'keen',\n",
       " 'Stick',\n",
       " 'bumpers',\n",
       " 'passionateness',\n",
       " 'prejudices',\n",
       " 'flashes',\n",
       " 'flies',\n",
       " 'heathen',\n",
       " 'son',\n",
       " 'Supreme',\n",
       " 'two',\n",
       " 'overleap',\n",
       " 'sinner',\n",
       " 'code',\n",
       " 'arrah',\n",
       " 'Copenhagen',\n",
       " 'Webster',\n",
       " 'HOLLAND',\n",
       " 'bubblingly',\n",
       " 'razor',\n",
       " 'detract',\n",
       " 'Algiers',\n",
       " 'headsmen',\n",
       " 'durable',\n",
       " 'deliberating',\n",
       " 'stooped',\n",
       " 'delights',\n",
       " 'shirt',\n",
       " 'cattle',\n",
       " 'managing',\n",
       " 'Who',\n",
       " 'goats',\n",
       " 'laugh',\n",
       " 'czar',\n",
       " 'pinch',\n",
       " 'proceed',\n",
       " 'matches',\n",
       " 'bedarned',\n",
       " 'abstinence',\n",
       " 'PAID',\n",
       " 'justify',\n",
       " 'every',\n",
       " 'marsh',\n",
       " 'twisting',\n",
       " 'handling',\n",
       " 'celestial',\n",
       " 'comber',\n",
       " 'butterfly',\n",
       " 'alpacas',\n",
       " 'sidelingly',\n",
       " 'blinds',\n",
       " 'waste',\n",
       " 'former',\n",
       " 'hoot',\n",
       " 'explained',\n",
       " 'Mohawk',\n",
       " 'snows',\n",
       " 'straight',\n",
       " 'passage',\n",
       " 'shameful',\n",
       " 'sounding',\n",
       " 'helplessly',\n",
       " 'importunity',\n",
       " 'discoveries',\n",
       " 'decent',\n",
       " 'unretracing',\n",
       " 'preparatives',\n",
       " 'Lesson',\n",
       " 'threading',\n",
       " 'Careful',\n",
       " 'Doom',\n",
       " 'yesterday',\n",
       " 'instead',\n",
       " 'me',\n",
       " 'prophesies',\n",
       " 'smackingly',\n",
       " 'fallacious',\n",
       " 'quailed',\n",
       " 'reiterating',\n",
       " 'armed',\n",
       " 'imaginations',\n",
       " 'duly',\n",
       " 'saying',\n",
       " 'treats',\n",
       " 'consign',\n",
       " 'BOUTON',\n",
       " 'with',\n",
       " 'Rhyme',\n",
       " 'inestimable',\n",
       " 'Bonapartes',\n",
       " 'events',\n",
       " 'Poles',\n",
       " 'anxious',\n",
       " 'Polar',\n",
       " 'tribulations',\n",
       " 'Screwed',\n",
       " 'lounge',\n",
       " 'eye',\n",
       " 'fantastic',\n",
       " 'figure',\n",
       " 'ridges',\n",
       " 'countrymen',\n",
       " 'nape',\n",
       " 'withdraws',\n",
       " 'mail',\n",
       " 'shallows',\n",
       " 'strays',\n",
       " 'fared',\n",
       " 'foamin',\n",
       " 'binding',\n",
       " 'Sabbath',\n",
       " 'incense',\n",
       " 'kissed',\n",
       " 'pants',\n",
       " 'Champollion',\n",
       " 'VOYAGES',\n",
       " 'cosy',\n",
       " 'improvement',\n",
       " 'public',\n",
       " 'unconcluded',\n",
       " 'Rondeletius',\n",
       " 'Plowdon',\n",
       " 'mildness',\n",
       " 'illness',\n",
       " 'flaming',\n",
       " 'WATCH',\n",
       " 'chock',\n",
       " 'voided',\n",
       " 'irresistible',\n",
       " 'Tropic',\n",
       " 'exegetist',\n",
       " 'vacancy',\n",
       " 'tongueless',\n",
       " 'fencing',\n",
       " 'rocks',\n",
       " 'leaving',\n",
       " 'seals',\n",
       " 'Queen',\n",
       " 'communicates',\n",
       " 'dwarfed',\n",
       " 'mischievous',\n",
       " 'shank',\n",
       " 'deliberately',\n",
       " 'Povelson',\n",
       " 'benevolently',\n",
       " 'peg',\n",
       " 'Venice',\n",
       " 'Arkansas',\n",
       " 'plumage',\n",
       " 'cruisings',\n",
       " 'symmetrically',\n",
       " 'Try',\n",
       " 'papers',\n",
       " 'fleece',\n",
       " 'fades',\n",
       " 'believers',\n",
       " 'weeds',\n",
       " 'hangman',\n",
       " 'REQUIN',\n",
       " 'archiepiscopacy',\n",
       " 'boasting',\n",
       " 'dere',\n",
       " 'WHICH',\n",
       " 'growl',\n",
       " 'roots',\n",
       " 'bestows',\n",
       " 'envy',\n",
       " 'matted',\n",
       " 'assembly',\n",
       " 'Rev',\n",
       " 'panics',\n",
       " 'shocked',\n",
       " 'brides',\n",
       " 'drifted',\n",
       " 'aunts',\n",
       " 'purposed',\n",
       " 'Scotch',\n",
       " 'upwards',\n",
       " 'initiate',\n",
       " 'assumed',\n",
       " 'BREACH',\n",
       " 'trending',\n",
       " 'unpleasant',\n",
       " 'hast',\n",
       " 'any',\n",
       " 'ONLY',\n",
       " 'intimation',\n",
       " 'Matter',\n",
       " 'Mt',\n",
       " 'post',\n",
       " 'Vacantly',\n",
       " 'cranial',\n",
       " 'indication',\n",
       " 'Will',\n",
       " 'notion',\n",
       " 'fetches',\n",
       " 'scamp',\n",
       " 'centaurs',\n",
       " 'mite',\n",
       " 'seizing',\n",
       " 'intermitting',\n",
       " 'Nod',\n",
       " 'unvitiated',\n",
       " 'chemistry',\n",
       " 'dividends',\n",
       " 'Whaleman',\n",
       " 'balena',\n",
       " 'deficiency',\n",
       " 'kicks',\n",
       " 'BROTHER',\n",
       " 'surround',\n",
       " 'children',\n",
       " 'keeled',\n",
       " 'crystal',\n",
       " 'wholly',\n",
       " 'depresses',\n",
       " 'harems',\n",
       " 'tulips',\n",
       " 'caulked',\n",
       " 'Mene',\n",
       " 'varying',\n",
       " 'succor',\n",
       " 'chip',\n",
       " 'circlings',\n",
       " 'gloating',\n",
       " 'generally',\n",
       " 'incrustation',\n",
       " 'lowermost',\n",
       " 'slippering',\n",
       " 'quits',\n",
       " 'allude',\n",
       " 'whirled',\n",
       " 'analogies',\n",
       " 'ultimate',\n",
       " 'triune',\n",
       " 'Spirit',\n",
       " 'slacken',\n",
       " 'SEBOND',\n",
       " 'Englishmen',\n",
       " 'Knife',\n",
       " 'Sail',\n",
       " 'rifle',\n",
       " 'climb',\n",
       " 'foam',\n",
       " 'henceforth',\n",
       " 'backs',\n",
       " 'jewelled',\n",
       " 'feasted',\n",
       " 'Nantuckois',\n",
       " 'dissect',\n",
       " 'oppositely',\n",
       " 'waifed',\n",
       " 'reclines',\n",
       " 'measureless',\n",
       " 'soaked',\n",
       " 'commands',\n",
       " 'warming',\n",
       " 'Merchant',\n",
       " 'rips',\n",
       " 'collated',\n",
       " 'from',\n",
       " 'Brace',\n",
       " 'ghostliness',\n",
       " 'marrow',\n",
       " 'everyway',\n",
       " 'represented',\n",
       " 'moulded',\n",
       " 'ringed',\n",
       " 'chimneys',\n",
       " 'Wrapping',\n",
       " 'squally',\n",
       " 'Eh',\n",
       " 'pirates',\n",
       " 'prevailing',\n",
       " 'influences',\n",
       " 'wag',\n",
       " 'secluded',\n",
       " 'falls',\n",
       " 'spat',\n",
       " 'maledictions',\n",
       " 'intact',\n",
       " 'accounting',\n",
       " 'office',\n",
       " 'should',\n",
       " 'bob',\n",
       " 'educated',\n",
       " 'tinkering',\n",
       " 'temptation',\n",
       " 'First',\n",
       " 'encircling',\n",
       " 'consecutive',\n",
       " 'inactive',\n",
       " 'incalculable',\n",
       " 'nailed',\n",
       " 'paw',\n",
       " 'speed',\n",
       " 'haughtily',\n",
       " 'potentates',\n",
       " 'fashioned',\n",
       " 'third',\n",
       " 'last',\n",
       " 'paralysed',\n",
       " 'garnished',\n",
       " 'inglorious',\n",
       " 'execute',\n",
       " 'pods',\n",
       " 'SINGING',\n",
       " 'Winding',\n",
       " 'smelling',\n",
       " 'stiver',\n",
       " 'Remember',\n",
       " 'hypocrisies',\n",
       " 'prosecution',\n",
       " 'bowlines',\n",
       " 'rowlocks',\n",
       " 'crafty',\n",
       " 'mysteries',\n",
       " 'pinny',\n",
       " 'New',\n",
       " 'RAPE',\n",
       " 'boiling',\n",
       " 'unavoidable',\n",
       " 'scouts',\n",
       " 'suckingly',\n",
       " 'Landlord',\n",
       " 'reader',\n",
       " 'Equality',\n",
       " 'coal',\n",
       " 'induced',\n",
       " 'both',\n",
       " 'whitest',\n",
       " 'tin',\n",
       " 'toothless',\n",
       " 'destiny',\n",
       " 'acquiescence',\n",
       " 'obstetrics',\n",
       " 'ducked',\n",
       " 'remained',\n",
       " 'plaid',\n",
       " 'Balaene',\n",
       " 'serpentine',\n",
       " 'style',\n",
       " 'Signals',\n",
       " 'TERROREM',\n",
       " 'complexioned',\n",
       " 'THRUSTS',\n",
       " 'Take',\n",
       " 'hardest',\n",
       " 'silence',\n",
       " 'specific',\n",
       " 'subsiding',\n",
       " 'Har',\n",
       " 'vials',\n",
       " 'Purchas',\n",
       " 'sockets',\n",
       " 'archangel',\n",
       " 'indubitably',\n",
       " 'sincerity',\n",
       " 'gamboge',\n",
       " 'Alexander',\n",
       " 'today',\n",
       " 'bald',\n",
       " 'errors',\n",
       " 'regular',\n",
       " 'mapped',\n",
       " 'roasted',\n",
       " 'Ignorance',\n",
       " 'diamond',\n",
       " 'manipulated',\n",
       " 'insular',\n",
       " 'banished',\n",
       " 'plummet',\n",
       " 'perfidious',\n",
       " 'health',\n",
       " 'Stars',\n",
       " 'difference',\n",
       " 'gettee',\n",
       " 'convivial',\n",
       " 'praying',\n",
       " 'Physiognomy',\n",
       " 'unquestionable',\n",
       " 'compliments',\n",
       " 'shamble',\n",
       " 'external',\n",
       " 'thereabouts',\n",
       " 'killed',\n",
       " 'tunic',\n",
       " 'heighten',\n",
       " 'blanket',\n",
       " 'comets',\n",
       " 'authorized',\n",
       " 'purge',\n",
       " 'clipped',\n",
       " 'hissing',\n",
       " 'senses',\n",
       " 'pounded',\n",
       " 'dried',\n",
       " 'sheep',\n",
       " 'that',\n",
       " 'royalties',\n",
       " 'TAKING',\n",
       " 'catarrhs',\n",
       " 'expandingly',\n",
       " 'conduits',\n",
       " 'brig',\n",
       " 'pearl',\n",
       " 'cupbearers',\n",
       " 'planned',\n",
       " 'grow',\n",
       " 'ply',\n",
       " 'original',\n",
       " 'window',\n",
       " 'sinecure',\n",
       " 'betokening',\n",
       " 'tornado',\n",
       " 'poltroon',\n",
       " 'abstemious',\n",
       " 'coffined',\n",
       " 'greenly',\n",
       " 'native',\n",
       " 'absorbing',\n",
       " 'barren',\n",
       " 'Isabella',\n",
       " 'improbable',\n",
       " 'agitated',\n",
       " 'seemingly',\n",
       " 'hams',\n",
       " 'noises',\n",
       " 'jot',\n",
       " 'marry',\n",
       " 'lingo',\n",
       " 'landsmen',\n",
       " 'lets',\n",
       " 'expressing',\n",
       " 'underlings',\n",
       " 'REMAINING',\n",
       " 'radiates',\n",
       " 'paved',\n",
       " 'inhabitant',\n",
       " 'Colonies',\n",
       " 'numbered',\n",
       " 'appal',\n",
       " 'spike',\n",
       " 'whooping',\n",
       " 'guess',\n",
       " 'mists',\n",
       " 'partitioned',\n",
       " 'Alleghanies',\n",
       " 'fertility',\n",
       " 'lava',\n",
       " 'butteries',\n",
       " 'juicy',\n",
       " 'able',\n",
       " 'heedfulness',\n",
       " 'encompassed',\n",
       " 'peeping',\n",
       " 'Louis',\n",
       " 'completing',\n",
       " 'idler',\n",
       " 'appears',\n",
       " 'pipes',\n",
       " 'pallid',\n",
       " 'domineer',\n",
       " 'ledge',\n",
       " 'burned',\n",
       " 'thankless',\n",
       " 'charged',\n",
       " 'Lifted',\n",
       " 'deafened',\n",
       " 'port',\n",
       " 'Danes',\n",
       " 'Woebegone',\n",
       " 'purely',\n",
       " 'vitals',\n",
       " 'wallowing',\n",
       " 'liar',\n",
       " 'rehearsed',\n",
       " 'Charley',\n",
       " 'deaths',\n",
       " 'wait',\n",
       " 'seventy',\n",
       " 'behead',\n",
       " 'quivering',\n",
       " 'Marten',\n",
       " 'squilgee',\n",
       " 'guides',\n",
       " 'ours',\n",
       " 'sell',\n",
       " 'pilgrims',\n",
       " 'outdone',\n",
       " 'suffocated',\n",
       " 'urchins',\n",
       " 'slept',\n",
       " 'Zeuglodon',\n",
       " 'withdraw',\n",
       " 'thro',\n",
       " 'Colnett',\n",
       " 'jumping',\n",
       " 'whizzings',\n",
       " 'Nightgown',\n",
       " 'flailed',\n",
       " 'alter',\n",
       " 'mutinous',\n",
       " 'remaining',\n",
       " 'Phrenologist',\n",
       " 'bannered',\n",
       " 'annually',\n",
       " 'mouts',\n",
       " 'solar',\n",
       " 'deviations',\n",
       " 'beheaded',\n",
       " ...]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def answer_six():\n",
    "    from itertools import compress\n",
    "    tokens = nltk.word_tokenize(moby_raw)\n",
    "    utokens = list(set(tokens))\n",
    "    utokens = list(compress(utokens, [x.isalpha() for x in utokens]))\n",
    "    uni_tokens = pd.Series(list(map(lambda x:tokens.count(x), utokens)), \n",
    "                           index = utokens)\n",
    "    return uni_tokens\n",
    "\n",
    "answer_six()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7\n",
    "\n",
    "What is the average number of tokens per sentence?\n",
    "\n",
    "*This function should return a float.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_seven():\n",
    "    \n",
    "    \n",
    "    return # Your answer here\n",
    "\n",
    "answer_seven()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 8\n",
    "\n",
    "What are the 5 most frequent parts of speech in this text? What is their frequency?\n",
    "\n",
    "*This function should return a list of tuples of the form `(part_of_speech, frequency)` sorted in descending order of frequency.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_eight():\n",
    "    \n",
    "    \n",
    "    return # Your answer here\n",
    "\n",
    "answer_eight()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - Spelling Recommender\n",
    "\n",
    "For this part of the assignment you will create three different spelling recommenders, that each take a list of misspelled words and recommends a correctly spelled word for every word in the list.\n",
    "\n",
    "For every misspelled word, the recommender should find find the word in `correct_spellings` that has the shortest distance*, and starts with the same letter as the misspelled word, and return that word as a recommendation.\n",
    "\n",
    "*Each of the three different recommenders will use a different distance measure (outlined below).\n",
    "\n",
    "Each of the recommenders should provide recommendations for the three default words provided: `['cormulent', 'incendenece', 'validrate']`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import words\n",
    "\n",
    "correct_spellings = words.words()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 9\n",
    "\n",
    "For this recommender, your function should provide recommendations for the three default words provided above using the following distance metric:\n",
    "\n",
    "**[Jaccard distance](https://en.wikipedia.org/wiki/Jaccard_index) on the trigrams of the two words.**\n",
    "\n",
    "*This function should return a list of length three:\n",
    "`['cormulent_reccomendation', 'incendenece_reccomendation', 'validrate_reccomendation']`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_nine(entries=['cormulent', 'incendenece', 'validrate']):\n",
    "    \n",
    "    \n",
    "    return # Your answer here\n",
    "    \n",
    "answer_nine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 10\n",
    "\n",
    "For this recommender, your function should provide recommendations for the three default words provided above using the following distance metric:\n",
    "\n",
    "**[Jaccard distance](https://en.wikipedia.org/wiki/Jaccard_index) on the 4-grams of the two words.**\n",
    "\n",
    "*This function should return a list of length three:\n",
    "`['cormulent_reccomendation', 'incendenece_reccomendation', 'validrate_reccomendation']`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_ten(entries=['cormulent', 'incendenece', 'validrate']):\n",
    "    \n",
    "    \n",
    "    return # Your answer here\n",
    "    \n",
    "answer_ten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 11\n",
    "\n",
    "For this recommender, your function should provide recommendations for the three default words provided above using the following distance metric:\n",
    "\n",
    "**[Edit distance on the two words with transpositions.](https://en.wikipedia.org/wiki/Damerau%E2%80%93Levenshtein_distance)**\n",
    "\n",
    "*This function should return a list of length three:\n",
    "`['cormulent_reccomendation', 'incendenece_reccomendation', 'validrate_reccomendation']`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_eleven(entries=['cormulent', 'incendenece', 'validrate']):\n",
    "    \n",
    "    \n",
    "    return # Your answer here \n",
    "    \n",
    "answer_eleven()"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "python-text-mining",
   "graded_item_id": "r35En",
   "launcher_item_id": "tCVfW",
   "part_id": "NTVgL"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1rc2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
